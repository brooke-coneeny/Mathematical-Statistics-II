---
title: "MCMC for Probit Regression"
author: "Brooke Coneeny"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, echo = FALSE, message = FALSE}
# Import Libraries 
library(tidyverse)
```

```{r data, echo = FALSE, message = FALSE}
# Load in data 
footballdata <- read.table("NFL2010_pres5.txt", head = T, sep = ",")
```

# Background Information

As we know, the objective of Bayesian data analysis is to determine the posterior distribution given the prior distribution and likelihood distribution. 

Markov Chain Monte Carlo methods allow us to estimate the shape of a posterior distribution in case we cannot compute it directly. In other words, MCMC methods are used to approximate the posterior distribution of a paramter of interest by random sampling in a probablistic space 


Monte Carlo simulations are a way of estimating a fixed parameter by repeatedly generating random numbers

Markov Chains are sequences of events that are probabilistically related to one another. Each event comes from a set of outcomes, and each outcome determines which outcome occurs next, according to a fixed set of probabilities. An important feature of Markov Chains is that they are memoryless, meaning everything that no new information comes from knowing the history of events

Using the MCMC method, we draw samples from the posterior distribution, and then compute statistics on the samples drawn. We then compare the values of parameters by computing how likely each value is to explain the data, given our prior beliefs. If a randomly generated parameter value is better than the last one, it is added to the chain of parameter values with a certain probability determined by how much better it is.

# Gibbs Sampling 
Gibbs Sampling is used when the joint distribution is not known explicitly or is difficult to sample from directly, but the conditional distribution of each variable is known and easier to sample from. 

Suppose we have a vector of parameters $\theta = (\theta_1 , \theta_2, ..., \theta_k)$ and we want to estimate the joint posterior distribution $p(\theta | X)$. 

Suppose we can find and draw random samples from all of the conditional distributions:
$p(\theta_1 | \theta_2 , ...,\theta_k,X)$
$p(\theta_2 | \theta_1 , ...,\theta_k,X)$
$...$
$p(\theta_k | \theta_1 , \theta_2,...,X)$

For this method, the Markov Chain is constructed by sampling from the conditional distribution for each parameter $\theta_i$, conditional on the current values of the other variables. Once we have iterated over all parameters, we completed one cycle of the Gibbs sampler.

## Gibbs Sampling for Probit Distribution 
We can use this MCMC method to carry out Bayesian inference for the Probit regression model. Instead of plugging in the MLE for the coefficient $\beta$, we assume prior distribution on $\beta$ and simulate from its posterior distribution. 

Recall that the Probit model may be expressed in terms of a latent variable $\theta_i = X^{`}_{i} \beta + Z_i$, where $Z_i \overset{iid}\sim N(0,1)$

We observe $Y_i = 1$ if $\theta_i \geq 0$ and $Y_i = 0$ otherwise, for $i = 1,...,n$. Then $P(Y_i = 1 | \beta) = \phi (X_{i}\beta)$. The non-informative prior $p(\beta) \propto c$ results in a posterior distribution that is proportional to the likelihood function.

In this example, our conditional distributions are: 
$p(Y_1 | Y_2 , ...,Y_k, \beta)$
$p(Y_2 | Y_1 , ...,Y_k, \beta)$
$...$
$p(Y_k | Y_1 , Y_2,..., \beta)$

# Gibbs Sampling Demonstration
We are going to use the NFL home wins and losses data, using halftime margin as an explanatory variable. This results in two parameters, $\beta_0$ and $\beta_1$. 

```{r probit, echo = FALSE, warnings = FALSE}
modeldata <- footballdata %>%
  mutate(response1 = case_when(
    Home.win == "home win" ~ 1,
    Home.win == "road win" ~ 0
  ))

probit_model <- glm(response1 ~ Halftime.H.R, data = modeldata,family = "binomial")
summary(model2)
```

```{r demonstration, echo = FALSE}
# initialize parameters 
B0 = 0
B1 = 0

# get value of n so we know how many iterations to perform 
n = nrow(modeldata)

# for n-1 iterations 
for (x in 0:(n-1)) {
  # sample each parameter from its conditional dist
}
```

#####
Compare the posterior mean estimate for $\beta$ to the MLE value
#####

# Interval Estimates
One advantage of Bayesian probit regression is that it is easy to generate interval estimates for the success probabilities. Each simulated $\beta$ implies a probability, and you can find the middle 95% of the simulated probabilities for each $x_i$ to represent 95% credible intervals. 

#####
Generate a graph of the probability curve vs. $x$ with 95% interval estimates drawn in
#####

#####
Report a 95% posterior interval for the probability that a home team leading by 7 points at halftime will win the game
#####

# Improprt Prior Distributions & Proper Posterior Distributions 
The improper prior distribution for $\beta$ will not give a proper posterior distribution for data that exhibit perfect separation.

Consider $x = c(-2,-1,0,1,2) and y = c(0,0,0,1,1)$

#####
Explain why the likelihood function cannot be normalized into a pdf
#####

#####
Show that the Gibbs sampler from part c will generate draws, but not meaningful draws (which is dangerous)
#####

#####
Explain why a $N_2(0,I)$ prior distribution (or any other proper distribution) for $\beta$ will yield a proper posterior distribution with these data 
##### 

#####
Run a Gibbs sampler and report posterior mean and variance estimates for $\beta$
#####


# Sources

https://towardsdatascience.com/a-zero-math-introduction-to-markov-chain-monte-carlo-methods-dcba889e0c50

https://people.duke.edu/~ccc14/sta-663/mcmc.html

https://towardsdatascience.com/gibbs-sampling-8e4844560ae5









