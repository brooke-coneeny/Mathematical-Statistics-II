pf(2.101, 6, 538, lower.tail = FALSE)
#Set 3 theta values and pick a known value for V
theta = c(0,1,2)
V=1
K=3
#Approximate expectation of the sum of squares
#Do this by averaging the observed sum of squares
#for a large number of simulated Y1,Y2,Y3 values
#using the given theta's as mean values
nsim = 100000
#Create sums of squares using Yi's and using stein estimate
ss0 = ss1 = rep(0,nsim)
ss0
#Conduct nsim simulations
for(iter in 1:nsim){
#generate y|theta,V ~ N(theta,V)
y =theta +  sqrt(V)*rnorm(k)
#y is a k-vector, with E(yi)=thetai, i=1,..,k
# save sum of squares for yi's
ss0[iter]=sum((y-theta)^2)
#Compute james-stein estimates
#k=3 so shrink towars mu = 0
Bhat = (k-2)*V/sum(y^2)
thetahat = Bhat*0 + (1-Bhat)*y
#Store sum of squares for thetahats
ss1[iter] = sum((thetahat - theta)^2)
#Print out every 1000th iteration
if(iter/1000 == round(iter/1000)) cat(iter,", ")
}
#Conduct nsim simulations
for(iter in 1:nsim){
#generate y|theta,V ~ N(theta,V)
y =theta +  sqrt(V)*rnorm(k)
#y is a k-vector, with E(yi)=thetai, i=1,..,k
# save sum of squares for yi's
ss0[iter]=sum((y-theta)^2)
#Compute james-stein estimates
#k=3 so shrink towars mu = 0
Bhat = (K-2)*V/sum(y^2)
thetahat = Bhat*0 + (1-Bhat)*y
#Store sum of squares for thetahats
ss1[iter] = sum((thetahat - theta)^2)
#Print out every 1000th iteration
if(iter/1000 == round(iter/1000)) cat(iter,", ")
}
#Conduct nsim simulations
for(iter in 1:nsim){
#generate y|theta,V ~ N(theta,V)
y =theta +  sqrt(V)*rnorm(K)
#y is a k-vector, with E(yi)=thetai, i=1,..,k
# save sum of squares for yi's
ss0[iter]=sum((y-theta)^2)
#Compute james-stein estimates
#k=3 so shrink towars mu = 0
Bhat = (K-2)*V/sum(y^2)
thetahat = Bhat*0 + (1-Bhat)*y
#Store sum of squares for thetahats
ss1[iter] = sum((thetahat - theta)^2)
#Print out every 1000th iteration
if(iter/1000 == round(iter/1000)) cat(iter,", ")
}
#Compare average sum of squares values
mean(ss0); mean(ss1)
#Compute ratio: ss1 should be smaller, so ratio should always be >1
mean(ss0)/mean(ss1)
#Set 4 theta values and pick a known value for V
theta = c(-1,0,1,2)
V=1
K=4
#Approximate expectation of the sum of squares
#Do this by averaging the observed sum of squares
#for a large number of simulated Y1,Y2,Y3,Y4 values
#using the given theta's as mean values
nsim = 100000
#Create sums of squares using Yi's and using stein estimate
ss0 = ss1 = rep(0,nsim)
#Conduct nsim simulations
for(iter in 1:nsim){
#generate y|theta,V ~ N(theta,V)
y = theta + sqrt(V)*rnorm(K)
#y is a k-vector, with E(yi)=thetai, i=1,..,k
# save sum of squares for yi's
ss0[iter]=sum((y-theta)^2)
#Compute james-stein estimates
ybar = mean(y)
Bhat = (K-3)*V/sum((y-ybar)^2)
thetahat = Bhat*ybar + (1-Bhat)*y
#Store sum of squares for thetahats
ss1[iter] = sum((thetahat - theta)^2)
#Print out every 1000th iteration
if(iter/1000 == round(iter/1000)) cat(iter,", ")
}
#Compare average sum of squares values
mean(ss0); mean(ss1)
#Compute ratio: ss1 should be smaller, so ratio should always be >1
mean(ss0)/mean(ss1)
#Set 4 theta values and pick a known value for V
theta = c(71.3,70.7,72.8,70.1)
V=6.0/5
K=4
#Approximate expectation of the sum of squares
#Do this by averaging the observed sum of squares
#for a large number of simulated Y1,Y2,Y3,Y4 values
#using the given theta's as mean values
nsim = 100000
#Create sums of squares using Yi's and using stein estimate
ss0 = ss1 = rep(0,nsim)
#Conduct nsim simulations
for(iter in 1:nsim){
#generate y|theta,V ~ N(theta,V)
y = theta + sqrt(V)*rnorm(K)
#y is a k-vector, with E(yi)=thetai, i=1,..,k
# save sum of squares for yi's
ss0[iter]=sum((y-theta)^2)
#Compute james-stein estimates
ybar = mean(y)
Bhat = (K-3)*V/sum((y-ybar)^2)
thetahat = Bhat*ybar + (1-Bhat)*y
#Store sum of squares for thetahats
ss1[iter] = sum((thetahat - theta)^2)
#Print out every 1000th iteration
if(iter/1000 == round(iter/1000)) cat(iter,", ")
}
#Compare average sum of squares values
mean(ss0); mean(ss1)
#Compute ratio: ss1 should be smaller, so ratio should always be >1
mean(ss0)/mean(ss1)
#Compute ratio: ss1 should be smaller, so ratio should always be >1
mean(ss0)/mean(ss1)
```
# Sources
https://statweb.stanford.edu/~joftius/slides/testing.pdf
Efrom & Morris 1977: Stein's Paradox
Everson - Steins Paradox
# make an empty graph with the correct limits
plot(0,0, xlim = c(-30,30), ylim = c(0,1), type = "n", xlab = "x (points)", ylab = "P(Y=1|x)")
# make an empty graph with the correct limits
plot(0,0, xlim = c(-30,30), ylim = c(0,1), type = "n", xlab = "x (points)", ylab = "P(Y=1|x)")
# add the two lines (change beta0 and beta1 to numbers from your fits)
lines(seq(-30,30,.1), 1/(1+exp(-(beta0+beta1*seq(-30,30,.1)))))
lines(seq(-30,30,.1), 1/(1+exp(-beta0+beta1*seq(-30,30,.1))), col="blue")
# make an empty graph with the correct limits
plot(0,0, xlim = c(-30,30), ylim = c(0,1), type = "n", xlab = "x (points)", ylab = "P(Y=1|x)")
# make an empty graph with the correct limits
plot(0,0, xlim = c(-30,30), ylim = c(0,1), type = "n", xlab = "x (points)", ylab = "P(Y=1|x)")
# add the two lines (change beta0 and beta1 to numbers from your fits)
lines(seq(-30,30,.1), 1/(1+exp(-(beta0+beta1*seq(-30,30,.1)))))
# make an empty graph with the correct limits
plot(0,0, xlim = c(-30,30), ylim = c(0,1), type = "n", xlab = "x (points)", ylab = "P(Y=1|x)")
# add the two lines (change beta0 and beta1 to numbers from your fits)
lines(seq(-30,30,.1), 1/(1+exp(-(beta0+beta1*seq(-30,30,.1)))))
lines(seq(-30,30,.1), 1/(1+exp(-beta0+beta1*seq(-30,30,.1))), col="blue")
model1
# Import Libraries
library(tidyverse)
# Load in data
footballdata <- read.table("NFL2010.txt", head = T, sep = ",")
setwd("C:/Users/bcone/Downloads/STAT111-Presentations/Presentation 4")
# Load in data
footballdata <- read.table("NFL2010.txt", head = T, sep = ",")
modeldata <- footballdata %>%
mutate(response1 = case_when(
Home.win == "home win" ~ 1,
Home.win == "road win" ~ 0
))
model1 <- glm(response1 ~ Vegas.H.R, data = modeldata)
model2 <- glm(response1 ~ Halftime.H.R, data = modeldata)
# get predicted values and log odds
modeldata <- modeldata %>%
mutate(predicted_values = predict(model1)) %>%
mutate(log_odds = log(predicted_values/(1- predicted_values)))
assumption_plot <- modeldata %>%
ggplot(aes(x = Vegas.H.R, y = log_odds)) +
geom_point()
# get predicted values and log odds
modeldata <- modeldata %>%
mutate(predicted_values2 = predict(model2)) %>%
mutate(log_odds_two = log(predicted_values2/(1- predicted_values2)))
assumption_plot2 <- modeldata %>%
ggplot(aes(x = Vegas.H.R, y = log_odds_two)) +
geom_point()
model1
model2
# add the two lines (change beta0 and beta1 to numbers from your fits)
# model 1
lines(seq(-30,30,.1), 1/(1+exp(-(0.49136+0.02817*seq(-30,30,.1)))))
# model 2
lines(seq(-30,30,.1), 1/(1+exp(-(0.54479+0.02523*seq(-30,30,.1)))), col="blue")
model3 <- glm(response1 ~ Vegas.H.R + Halftime.H.R, data = modeldata)
model3
0.50913 + 0.01541(7) + 0.02320(3)
0.50913 + (0.01541*7) + (0.02320*3)
model3
summary(model3)
summary(mmodel1)
model1 <- glm(response1 ~ Vegas.H.R, data = modeldata)
summary(model1)
model2 <- glm(response1 ~ Halftime.H.R, data = modeldata)
summary(model2)
# make an empty graph with the correct limits
plot(0,0, xlim = c(-30,30), ylim = c(0,1), type = "n", xlab = "x (points)", ylab = "P(Y=1|x)")
# add the two lines (change beta0 and beta1 to numbers from your fits)
# model 1
lines(seq(-30,30,.1), 1/(1+exp(-(0.49136+0.02817*seq(-30,30,.1)))))
# model 2
lines(seq(-30,30,.1), 1/(1+exp(-(0.54479+0.02523*seq(-30,30,.1)))), col="blue")
125.74107 - 110.34566
(15.39541*(-2))
# make an empty graph with the correct limits
plot(0,0, xlim = c(-30,30), ylim = c(0,1), type = "n", xlab = "x (points)", ylab = "P(Y=1|x)")
# add the two lines (change beta0 and beta1 to numbers from your fits)
# model 1
lines(seq(-30,30,.1), 1/(1+exp(-(-0.0345715+0.12389873*seq(-30,30,.1)))))
# model 2
lines(seq(-30,30,.1), 1/(1+exp(-(0.27693564+0.16516549*seq(-30,30,.1)))), col="blue")
0.034571/0.1422308
0.27693564/0.1556258
This results in the following model:
$y_i$ = 0.50913 + 0.01541(Vegas.H.R) + 0.02320(Halftime.H.R)
(0.50913 + (0.01541*7) + (0.02320*3)
0
0.50913 + (0.01541*7) + (0.02320*3)
e^(-.6866)
exp(-.6866)
1+0.5032843
1/1.503284
model2 <- glm(response1 ~ Halftime.H.R, data = modeldata, family = "binomial")
summary(model2)
modeldata <- footballdata %>%
mutate(response1 = case_when(
Home.win == "home win" ~ 1,
Home.win == "road win" ~ 0
))
model1 <- glm(response1 ~ Vegas.H.R, data = modeldata, family = "binomial")
summary(model1)
model3 <- glm(response1 ~ factor(Halftime.H.R), data = modeldata, family = "binomial")
summary(model3)
model4 <- glm(response1 ~ factor(Halftime.H.R), data = modeldata, family = "binomial")
summary(model4)
model3 <- glm(response1 ~ Vegas.H.R + Halftime.H.R, data = modeldata)
summary(model3)
# Import Libraries
library(tidyverse)
# Load in data
footballdata <- read.table("NFL2010.txt", head = T, sep = ",")
setwd("C:/Users/bcone/Downloads/STAT111-Presentations/Presentation 4")
# Load in data
footballdata <- read.table("NFL2010.txt", head = T, sep = ",")
model3 <- glm(response1 ~ Vegas.H.R + Halftime.H.R, data = modeldata, family = "binomial")
summary(model3)
0.05103 + (0.10072*7) + (0.16219*3)
exp(1.24264)
1+3.464748
1/4.464748
assumption_plot2 <- modeldata %>%
ggplot(aes(x = Halftime.H.R, y = log_odds_two)) +
geom_point()
assumption_plot2
model5 <- glm(response1 ~ Vegas.H.R + Halftime.H.R, data = modeldata, family = "binomial")
summary(model5)
