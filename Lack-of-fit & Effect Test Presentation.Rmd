---
title: "Extra Sum of Squares and Lack of Fit Tests for ANCOVA"
author: "Brooke Coneeny"
output:
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = TRUE) 
```

```{r libraries, echo = FALSE, message = FALSE}
# Import Libraries 
library(tidyverse)
```

```{r data, echo = FALSE, message = FALSE}
# Load in data 
soccer_data <- read.delim("soccer_data.txt")
```

The data set we are going to use for this lecture revolves around the game of soccer. It includes a male soccer players weight, height and position

# Saturated Model
The linear models we have focused on have taken on the form $y_i = \beta_0 + \beta_1 x_{i,1} + ... + \beta_{p-1} x_{i,p-1} + \epsilon_i$ for $i = 1,...,n$. Using our soccer data, our linear model for regressing weight on height would take the form: $y_i = \beta_0 + \beta_1 (height) + \epsilon_i$. 

We are going to direct our focus to multiple regressions that model data sets involving a categorical explanatory variable. These variables can be described as $k$ distinct $x_{i}$ values. In the case of our soccer example, the categorical variable is $\textit{positions}$ which can be broken into distinct categories (midfield, defense, forward, and goalkeepers). Breaking up positions in this way gives us our $\textit{Saturated Model}$, which can represented by the following:

$y_{i} = \beta_0 + \beta_1 height + \beta_2 (defense) + \beta_3 (forward) + \beta_4 (goal keepers) + \epsilon_i$

$E(Y_i) = \begin{cases} \beta_0 + \beta_1 height \\ \beta_0 + \beta_1 height + \beta_2 (defense)\\ \beta_0 + \beta_1 height + \beta_3 (forward) \\ \beta_0 + \beta_1 height + \beta_4 (goal keepers) \end{cases}$

# Lack of Fit Output

A lack of fit test is used to asses the quality of saturated models, meaning we want to assess if the fitted model adequately models the response variable. In this case, we want to know how well our above model predicts a player's weight. When performing this test, the sum of squares error is separated into two components: $\textit{pure error}$ and $\textit{lack-of-fit error}$. 

### Pure Error
The pure error represents, assuming a normal model, the error within each group. This error is assumed to be constant throughout all groups. In the case of our soccer example, the error within defense, forward, midfield, and goal keepers is assumed to be the same. 

This error can be found by $\sum_{i=1}^{k}\sum_{j=1}^{n_{i}} (y_{ij} - \bar y)^2 = SS_{pure}$. Where $k$ is the number of distinct combinations of $x_{i}$'s and $n_i$ is the number of observations within the $i^{th}$ combination of variables. This means each $y_{ij}$ represents a $j^{th}$ observation in a group $i$. We can also say, under the null hypothesis (the linear model accurately predicts the expected value of $y_{i}$ and follows the assumptions of a normal model), that $(SS_{pure} / \sigma ^2) \sim X^2_{n-k}$, with $k$ being the number of distinct $x_{i}'s$. 

### Lack-of-Fit Error
The lack-of-fit error represents the error of the saturated model. This error is calculated by finding the distance of a group mean from the corresponding line estimate. 

This error can be found by the following: $\sum_{i=1}^k (\bar y_{i} - \hat y_{i})^2 = SS_{lack-of-fit}$. Where $k$ is the number of distinct combinations of $x_{i}$'s. We can also say, under the same null hypothesis as above, that $(SS_{lack-of-fit} / \sigma ^2) \sim X^2_{k-p}$, with $k$ being the number of distinct $x_{i}'s$ and $p$ being the number of parameters. 

### Max R-Squared
The max $R^{2}$ can be used to asses how well the saturated model fits the data. The formula for this value is: $1 - \frac{SS_{pure}}{SS_{total}}$. In our soccer example, using this equation we get $1 - \frac{40390.378}{46089.311} = 0.124$

### F-test
Using our knowledge of the pure error and lack-of-fit error from above, we can now construct an $F$ test for our saturated mode under the following hypotheses: 
$H_{0}:$ The relationship assumed in the saturated model is reasonable, there is no lack of fit 
$H_{a}:$ The relationship assumed in the model is not reasonable, there is lack of fit

The distribution for $F$ statistic can be found by:
$(\frac{MS_{lack-of-fit}}{MS_{pure error}}) = \frac{ \frac{SS_{lack-of-fit}}{k-p} } {\frac{SS_{pure error}}{n-k}} \sim F_{(k-p,n-k)}$, where $k$ is once again the number of distinct $x_{i}'s$ and $p$ is the number of parameters. 

Normally, we are looking for a large F-statistic, along with a small p value, in order to reject the null hypothesis. However, a large value of this $F$ statistic would provide evidence that the linearity assumption fails and there is a lack of fit in our mode. We therefore do not want a large value of F (we do not want to reject the null hypothesis). Also note that failing to reject the null hypothesis is $\textbf{NOT}$ proof that the normal linear model is correct! It simply means our model predicts better than the intercept-only model. 

Redirecting our attention to the soccer example once again, let us calculate the F-Statistic. The $MS_{lack-of-fit} = 123.890$ and the $MS_{pure error} = 115.732$, giving us $123.890 / 115.732 = 1.0705$, we as we can see from the output table is not significant. This means we do not reject the null hypothesis and we can assume the saturated model is reasonable. 

# Effect F Test Output 
The effect test is useful when dealing with models that include a categorical explanatory variable. It allows us to determine the significance of the variable as a whole, opposed to individual t-tests for the different variables within the category. 

## Output 
To make this more clear, let us direct our attention to the sample output gathered from our soccer data model. 

### Connections Between the T-test and Effect Test: Continuous Variable
We are going to specifically examine the differences between the tests, focusing on our continuous variable: height 

T-test: 
$H_0: \beta_1 = 0$
$H_a: \beta_1 \neq 0$
The result of this test can be found in the parameter estimate table. The p value is $<0.0001$, which means it is significant. Therefore, given that we have the other predictors, the addition of height in our model would be valuable. 

Effect F-test:
The null hypothesis for the effect f-test of height is very similar to the above t-test. It is testing, given we have a model containing position indicator variables, does adding height help to predict weight more accurately? 

The result of this test can be found in the effect tests table. The p value is $<0.0001$, and is therefore significant. This tells us that, given we have a model with position indicator variables, the addition of height in our model would be valuable. 

It is important to note that the t-test and partial f-test for height are testing the same hypothesis and that the f-statistic (358.8653) is the squared value of the t-statistic (18.94).

### Connections Between the T-test and Effect Test: Categorical Variable
Now let us direction our attention to the difference between the individual t-tests for each of the indicator variables and the effect test of the position variable as a whole.

In our parameter estimate table we can see there is a t-test performed for each position indicator. Looking at the goalkeeper variable specifically, our t-test is testing: 
$H_0: \beta_4 = 0$
$H_a: \beta_4 \neq 0$
As we can see from our table, the result was significant (p-value = 0.0105), telling us that including the goal keeper indicator in our model is beneficial. 

In our effect tests output, we can see that there is a f-test performed for the position variable as a whole. This test is different than performing a t-test for each position indicator. 

This f-test is testing, given a model with height as an explanatory variable, is the addition of all position indicators (as a whole) useful for predicting the weight of male soccer players? We can see from the results that the p-value (0.0021) is less than 0.05, and therefore significant. The f-test informs us that although not all of the position indicators were significant for their individual t-tests, the inclusion of these predictors in the model as a whole is significant. 

### Extra Sum of Squares F Statistic
To find the extra sum of squares f-statistic for position, we first need to find the sum of squares for position. 
$SS_{position} = SSE_{reduced} - SSE_{saturated} = (1-(r^2)SST) - SSE_{saturated} = (1-(0.7266)^{2}101333.75) - 46089.31 = 1741.5$

# Interaction Model
We have been using a saturated model throughout this lecture which includes a height predictor alongside indicator variables for each of the positions in soccer. Let us now examine an interaction model, using the same data set. 

### Why Add Interactions
Interactions are used in models when the extent to which $y_i$ is associated with a change in an $x_i$ depends on the value of a different $x_i$. In the case of our soccer example, we are creating an interaction between height and position because we believe some of the relationship between weight and height is dependent on position. To create an interaction term, we simply multiply the two explanatory variables together. 

Adding interactions to our above model would give us the following $y_i$'s: 
$Y_i = \beta_0 + \beta_1 height + \beta_2 (defense) + \beta_3 (forward) + \beta_4 (goal keepers) + \beta_5 (defense * height) + \beta_6 (forward * height) + \beta_7 (goalkeepers * height)$

The mean for individuals in each position are: 
$E(Y_i) = \begin{cases} \beta_0 + \beta_1 height \\ (\beta_0 + \beta_2) + (\beta_1 + \beta_5) height \\ (\beta_0 + \beta_3) + (\beta_1 + \beta_6)height\\ (\beta_0 + \beta_4) + (\beta_1 + \beta_7)height\end{cases}$

### Extra Sum of Squares F Statistic
Similar to the work we have done above, we are going to perform an effect f-test on our model. An effect f-test looks at a subset of explanatory variables and assess whether they are useful for predicting the response variable. In this case, the effect f-test will determine whether the subset of interaction variables are beneficial when predicting the weight of male soccer players. 

The f-statistic for this test is equal to $\frac{(\frac{SS_{extra}}{p})}{(\frac{SS_{error}}{n-p})}$. Using this equation, we get the following calculations:
$SS_{extra} = 187.44$
$\frac{SS_{extra}}{p} = 187.44 /3 = MS_{extra} = 62.381$
$SS_{error} = 45902.17$
$\frac{SS_{error}}{n-p} = 45902.17 / (400 - 8) = 117.097$
This gives us an f-stat = $\frac{62.381}{117.097} = 0.5327$. This directly matches the f-statistic in the effect test output, confirming our calculations. As we can see in the output, this f-statistic is not significant. This means we cannot reject the null hypothesis that the $\beta_{i}$\s for the interaction terms are all 0 ($H_{0} : \beta_5 = \beta_6 = \beta_7 = 0$). Using an f-test in this example, opposed to a t-test, was helpful because it allowed us to determine if the interactions as a whole were significant, whereas a t-test could only look at the terms individually. 

